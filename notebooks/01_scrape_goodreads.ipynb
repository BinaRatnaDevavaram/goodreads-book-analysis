{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 100 books on page 1\n",
      "Scraping page 2...\n",
      "Found 100 books on page 2\n",
      "Scraping page 3...\n",
      "Found 100 books on page 3\n",
      "Scraping page 4...\n",
      "Found 100 books on page 4\n",
      "Scraping page 5...\n",
      "Found 100 books on page 5\n",
      "Scraping page 6...\n",
      "Found 100 books on page 6\n",
      "Scraping page 7...\n",
      "Found 100 books on page 7\n",
      "Scraping page 8...\n",
      "Found 100 books on page 8\n",
      "Scraping page 9...\n",
      "Found 100 books on page 9\n",
      "Scraping page 10...\n",
      "Found 100 books on page 10\n",
      "✅ Scraping complete. Data saved to: data/goodreads_books.csv\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------\n",
    "# 📚 Import necessary libraries\n",
    "#-------------------------------------------------------------------\n",
    "# This script scrapes the Goodreads \"Best Books Ever\" list and saves it to a CSV file.\n",
    "# It uses:\n",
    "# 📡 requests: Used to send HTTP requests to web pages and get the raw HTML response\n",
    "# 🍜 BeautifulSoup: Parses HTML and XML documents, making it easy to extract data from web pages\n",
    "# 📊 pandas: For storing, manipulating, and exporting structured data in DataFrame format\n",
    "# ⏱️ time: Adds delays (like sleep) to avoid overloading servers or getting blocked\n",
    "# 🗂️ os: Interacts with the operating system, such as creating folders or checking file paths\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 📂 Ensure data folder exists\n",
    "def create_data_folder(path='../data'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "# 🌐 Generate the Goodreads list URL\n",
    "def get_page_url(page_num):\n",
    "    BASE_URL = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}\"\n",
    "    return BASE_URL.format(page_num)\n",
    "\n",
    "# 🤖 Mimic a browser with headers\n",
    "def get_headers():\n",
    "    return {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
    "            }\n",
    "    \n",
    "\n",
    "# 🧠 Extract book data from a BeautifulSoup row\n",
    "def extract_book_data(row):\n",
    "    try:\n",
    "        title = row.find('a', class_='bookTitle').get_text(strip=True)\n",
    "        author = row.find('a', class_='authorName').get_text(strip=True)\n",
    "        rating_text = row.find('span', class_='minirating').get_text(strip=True)\n",
    "\n",
    "        parts = rating_text.split(' — ')\n",
    "        avg_rating = float(parts[0].split()[0])\n",
    "        num_ratings = int(parts[1].split()[0].replace(',', ''))\n",
    "\n",
    "        return {\n",
    "            'Title': title,\n",
    "            'Author': author,\n",
    "            'Avg Rating': avg_rating,\n",
    "            'Num Ratings': num_ratings\n",
    "                }\n",
    "    \n",
    "    # If there are Exceptions and any data extraction fails, return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 🕷️ Scrape a single page and return list of book dictionaries\n",
    "def scrape_page(page_num):\n",
    "    print(f\"\\n🔄 Scraping page {page_num} of 100...\")\n",
    "    url = get_page_url(page_num)\n",
    "    response = requests.get(url, headers=get_headers())\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_rows = soup.find_all('tr', itemtype=\"http://schema.org/Book\")\n",
    "    print(f\"📚 Found {len(book_rows)} books on page {page_num}\")\n",
    "\n",
    "\n",
    "    page_books = []\n",
    "    for row in book_rows:\n",
    "        book_data = extract_book_data(row)\n",
    "        if book_data:\n",
    "            page_books.append(book_data)\n",
    "\n",
    "    # ⏱️ Be respectful to Goodreads\n",
    "    time.sleep(2)  \n",
    "    return page_books\n",
    "\n",
    "# 🗃️ Main function to scrape all pages and save CSV\n",
    "def scrape_goodreads_books(pages=100, output_path='../data/goodreads_books_scraped.csv'):\n",
    "    create_data_folder()\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        all_books.extend(scrape_page(page))\n",
    "\n",
    "    df = pd.DataFrame(all_books)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ Scraping complete. Data saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# ▶️ Run this script directly\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_goodreads_books(pages=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186a2de-f1b0-46e6-96a9-46661fafed1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
